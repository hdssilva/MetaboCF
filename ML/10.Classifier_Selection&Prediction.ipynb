{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fe981cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rcParams\n",
    "from sklearnex import patch_sklearn #Improves sklearn alghoritms performance\n",
    "patch_sklearn()\n",
    "import sklearn\n",
    "print('scikit-learn version\\n', sklearn.__version__)\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score,  f1_score, precision_score, classification_report, balanced_accuracy_score, confusion_matrix\n",
    "import seaborn as sns\n",
    "from aux_functions.name_2lines import name_2lines\n",
    "from aux_functions.make_plots import plot_confusion_matrix, OOMFormatter\n",
    "from aux_functions.prediction import get_class_w_prob\n",
    "from itertools import product\n",
    "import pickle\n",
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "import dtale\n",
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9aa91d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set seed\n",
    "seed=41"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be1f0887",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = pickle.load(open('Dataset/feature_names.pkl', 'rb'))\n",
    "X_test = pickle.load(open(f'Dataset/X_test.pkl', 'rb'))\n",
    "X_test_scal = pickle.load(open(f'Dataset/X_test_scal.pkl', 'rb'))\n",
    "y_test = pickle.load(open(f'Dataset/y_test.pkl', 'rb'))\n",
    "y_train = pickle.load(open(f'Dataset/y_train.pkl', 'rb'))\n",
    "all_categories = pickle.load(open('Dataset/all_categories.pkl', 'rb')) #all classes in the dataset\n",
    "sc_u = pickle.load(open(f'Dataset/SC_wunique_child.pkl', 'rb')) #unique superclasses\n",
    "c_u = pickle.load(open(f'Dataset/C_wunique_child.pkl', 'rb')) #unique classes\n",
    "kings, sclasses, classes, subclasses = all_categories['Kingdom'], all_categories['Superclass'], all_categories['Class'], all_categories['Subclass'] #nodes in each level\n",
    "\n",
    "#Set parent classes of the respective level, and which of them have only one child (doesn't have cf)\n",
    "hierarchy = {'Kingdom':{'classes': ['Chemical entities'], 'uniques':[]}, 'Superclass':{'classes': kings, 'uniques': []},\n",
    "     'Class':{'classes': sclasses, 'uniques': sc_u}, 'Subclass': {'classes': classes, 'uniques': c_u}}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b982b13",
   "metadata": {},
   "source": [
    "### Join GSCV results from all multiclass classifiers into a DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "781359d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add  results to a DataFrame which will contain all of GSCV scores of multiclass classifiers\n",
    "results = []\n",
    "    \n",
    "for level in ['Kingdom', 'Superclass', 'Class', 'Subclass']:\n",
    "    cls = hierarchy[level]['classes']\n",
    "    uniques = hierarchy[level]['uniques']\n",
    "    for i, class_ in enumerate(cls):\n",
    "        if class_ in uniques:\n",
    "            continue\n",
    "        elif level == 'Kingdom':\n",
    "            node_name = f'({level})'\n",
    "            i = ''\n",
    "        else:\n",
    "            node_name = f'({level}) {class_}'\n",
    "            i = f'{i}_'\n",
    "        for f_sel in ['all', 'sel']:\n",
    "            for alg in ['RF', 'KNN', 'LR', 'SVM', 'NB']:\n",
    "                gs = pickle.load(open(f'Models/{level}/{i}{alg}_{f_sel}_feat.pkl', 'rb'))\n",
    "                results.append({'Node': node_name, 'Feature selection': f_sel, 'Algorithm':alg,\n",
    "                                 'F1-Score-Macro (Val)': gs.cv_results_['mean_test_f1_macro'][gs.best_index_],\n",
    "                                 'F1-Score-Macro (Train)': gs.cv_results_['mean_train_f1_macro'][gs.best_index_],\n",
    "                                 'F1-Score-Micro (Val)': gs.cv_results_['mean_test_f1_micro'][gs.best_index_],\n",
    "                                 'F1-Score-Micro (Train)': gs.cv_results_['mean_train_f1_micro'][gs.best_index_]})\n",
    "results = pd.DataFrame(results)\n",
    "results.to_pickle('Results/Multiclass_GSCV_scores.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85654515",
   "metadata": {},
   "source": [
    "### Class vs Subclass classifiers performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49d0f498",
   "metadata": {},
   "outputs": [],
   "source": [
    "h = pd.DataFrame(y_test, columns=['Kingdom', 'Superclass', 'Class', 'Subclass']).sort_values(by=['Kingdom', 'Superclass', 'Class', 'Subclass'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29518af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "h[['Superclass', 'Class']].drop_duplicates()['Superclass'].value_counts().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "395d258d",
   "metadata": {},
   "outputs": [],
   "source": [
    "h[['Class', 'Subclass']].drop_duplicates()['Class'].value_counts().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c01e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "h['Superclass'].value_counts().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9580301",
   "metadata": {},
   "outputs": [],
   "source": [
    "h['Class'].value_counts().mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ac75360",
   "metadata": {},
   "source": [
    "## Compare algorithms' performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12af10c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.read_pickle('Results/Multiclass_GSCV_scores.pkl')\n",
    "count = []\n",
    "for node in results['Node'].drop_duplicates().values:\n",
    "    print(node)\n",
    "    filter_ = results[results['Node']==node]\n",
    "    alg = filter_.loc[filter_['F1-Score-Macro (Val)'].idxmax(), :]['Algorithm']\n",
    "    score = filter_.loc[filter_['F1-Score-Macro (Val)'].idxmax(), :]['F1-Score-Macro (Val)']\n",
    "    filter_score = filter_.loc[filter_['F1-Score-Macro (Val)']==score]\n",
    "    print(filter_score['Algorithm'].drop_duplicates().values)\n",
    "    count.extend(filter_score['Algorithm'].drop_duplicates().values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e49ff1d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "total = len(results['Node'].drop_duplicates().values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f085294d",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(count, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aca48ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "95*100/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10d52e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "37*100/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee89df7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "24*100/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfce1847",
   "metadata": {},
   "outputs": [],
   "source": [
    "18*100/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26b962c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "7*100/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8efe839",
   "metadata": {},
   "outputs": [],
   "source": [
    "### NB is taken out because there is always another algorithm with the same score\n",
    "### SVM does not return probability of prediction estimates\n",
    "###Superclass - Inorganic\n",
    "###75.13 - macro RF\n",
    "###91.75 - micro RF\n",
    "###76.13 - macro SVM\n",
    "###93.72 - micro SVM\n",
    "###Subclass\n",
    "###Azolydines 0.8088/0.8061 (LR)\n",
    "###Linear 1,3-diarylpropanoids 0.6030/0.5995 (KNN)\n",
    "###Miscellaneous mixed metals/non-metals equal to the 4th decimal\n",
    "###Triazines 0.6012/0.5935 (LR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09754989",
   "metadata": {},
   "source": [
    "## Select classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16ba5d59",
   "metadata": {},
   "source": [
    "All algorithms. Ignores NB and SVM. NB always has other algorithms with the same value, SVM does not output proba.\n",
    "For SVM, when there is no other algorithm with the same score, chooses the next best algorithm\n",
    "Na escolha entre algoritmos, dar prioridade (RF, KNN, LR), para nÃ£o haver tanta heterogeneidade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01e6fcda",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.read_pickle('Results/Multiclass_GSCV_scores.pkl')\n",
    "results = results[results['Algorithm'].isin(['RF', 'KNN', 'LR'])]\n",
    "level_scores = {}\n",
    "category_size = pd.Series(y_train.flat).value_counts()\n",
    "for level in ['Kingdom', 'Superclass', 'Class', 'Subclass']:\n",
    "    cls = hierarchy[level]['classes']\n",
    "    uniques = hierarchy[level]['uniques']\n",
    "    print(level)\n",
    "    macro = 0\n",
    "    macro_n = 0\n",
    "    micro = 0\n",
    "    micro_n = 0\n",
    "    for i, class_ in enumerate(cls):\n",
    "        if class_ in uniques:\n",
    "            continue\n",
    "        elif level == 'Kingdom':\n",
    "            node_name = f'({level})'\n",
    "            load_i = ''\n",
    "            dump_i = ''\n",
    "        else:\n",
    "            node_name = f'({level}) {class_}'\n",
    "            load_i = f'{i}_'\n",
    "            dump_i = f'_{i}'\n",
    "        print(class_)\n",
    "        #Chooses the best algorithm/f_sel combination\n",
    "        score = results.loc[results.loc[results['Node']==node_name, 'F1-Score-Macro (Val)'].idxmax(), 'F1-Score-Macro (Val)']\n",
    "        filter_ = results.loc[(results['Node']==node_name) & (results['F1-Score-Macro (Val)']==score)]\n",
    "        #If there is more than one combination\n",
    "        if len(filter_) > 1:\n",
    "            #Priority to algorithms, by order RF, KNN, LR\n",
    "            algs = filter_['Algorithm'].drop_duplicates().values\n",
    "            for alg in ['RF', 'KNN', 'LR']:\n",
    "                if alg in algs:\n",
    "                    #Priority to no feature selection == 'all'\n",
    "                    if 'all' in filter_[filter_['Algorithm']==alg]['Feature selection'].values:\n",
    "                        f_sel = 'all'\n",
    "                    else:\n",
    "                        f_sel = 'sel'\n",
    "                    print(alg, f_sel)\n",
    "                    break\n",
    "        else:\n",
    "            alg = filter_.iloc[0].loc['Algorithm']\n",
    "            f_sel = filter_ .iloc[0].loc['Feature selection']\n",
    "        \n",
    "        gs = pickle.load(open(f'Models/{level}/{load_i}{alg}_{f_sel}_feat.pkl', 'rb'))\n",
    "        pickle.dump(gs.best_estimator_, open(f'Selected_classifiers/{level}{dump_i}.pkl', 'wb'))\n",
    "        \n",
    "        scores = results[(results['Node']==node_name) & (results['Algorithm']==alg) & (results['Feature selection']==f_sel)].iloc[0]\n",
    "        macro += scores['F1-Score-Macro (Val)']*len(gs.best_estimator_.classes_)\n",
    "        macro_n += len(gs.best_estimator_.classes_)\n",
    "        if class_ == 'Chemical entities':\n",
    "            micro += scores['F1-Score-Micro (Val)']*(category_size['Organic compounds'] + category_size['Inorganic compounds'])\n",
    "            micro_n += (category_size['Organic compounds'] + category_size['Inorganic compounds'])\n",
    "        else:\n",
    "            micro += scores['F1-Score-Micro (Val)']*category_size[class_]\n",
    "            micro_n += category_size[class_]\n",
    "    level_scores[level] = {'macro_f1_score':macro/macro_n, 'micro_f1_score':micro/micro_n}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e74921e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Performance of the classifiers at each level, based on local score from cross_validation\n",
    "pd.DataFrame(level_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b678064a",
   "metadata": {},
   "source": [
    "## Validation on Test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e27c477",
   "metadata": {},
   "source": [
    "### Load the classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d47ee4d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfs = {}\n",
    "for level in ['Kingdom', 'Superclass', 'Class', 'Subclass']:\n",
    "    print(level)\n",
    "    cls = hierarchy[level]['classes']\n",
    "    uniques = hierarchy[level]['uniques']\n",
    "    cfs[level] = {}\n",
    "    for i, class_ in tqdm(enumerate(cls), total=len(cls)):\n",
    "        if class_ in uniques:\n",
    "            continue\n",
    "        elif level == 'Kingdom':\n",
    "            i = ''\n",
    "        else:\n",
    "            i = f'_{i}'\n",
    "        with open(f'Selected_classifiers/{level}{i}.pkl', 'rb') as f:\n",
    "            cf = pickle.load(f)\n",
    "            if cf.n_features_in_ != 133:\n",
    "                if level == 'Kingdom':\n",
    "                    sel_f = pickle.load(open('Results/selected_features.pkl', 'rb'))[level]['selected features']\n",
    "                else:\n",
    "                    sel_f = pickle.load(open('Results/selected_features.pkl', 'rb'))[level][class_]['selected features']\n",
    "                f_index = [feature_names.index(feature) for feature in sel_f]\n",
    "            else:\n",
    "                f_index = list(range(len(feature_names)))\n",
    "            cfs[level][class_] = (cf, f_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e82500a4",
   "metadata": {},
   "source": [
    "### Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1abfbb24",
   "metadata": {},
   "outputs": [],
   "source": [
    "levels = ['Kingdom', 'Superclass', 'Class', 'Subclass']\n",
    "\n",
    "#Kingdom prediction\n",
    "cf, fi = cfs['Kingdom']['Chemical entities']\n",
    "#first level prediction is joined into a df\n",
    "#decision of scaling or not is done inside this function (based if it is RF or not)\n",
    "y_pred = get_class_w_prob(cf, 'Kingdom', X_test[:, fi], X_test_scal[:, fi])\n",
    "\n",
    "#Sublevels prediction\n",
    "for i, level in enumerate(levels):\n",
    "    if i==0:\n",
    "        continue\n",
    "    print(level)\n",
    "    #classes that do not need classifier\n",
    "    uniques = hierarchy[level]['uniques']\n",
    "    #df for the prediciton of the whole level\n",
    "    new_pred = pd.DataFrame()\n",
    "    level_cfs = cfs[level]\n",
    "    #iterate each parent node to make predictions on the whole level\n",
    "    for class_ in tqdm(level_cfs.keys(), desc='Classifiers prediction'):\n",
    "        cf, fi = level_cfs[class_]\n",
    "        #index of the samples to be classified (the ones with the parent node predicted at previous level)\n",
    "        index = y_pred[y_pred[f'{levels[i-1]} pred']==class_].index\n",
    "        if np.shape(X_test[index])[0] == 0: #There is nothing to predict\n",
    "            continue\n",
    "        y_pred_class = get_class_w_prob(cf, level, X_test[index][:, fi], X_test_scal[index][:, fi], index=index)\n",
    "        new_pred = pd.concat([new_pred, y_pred_class], axis=0)\n",
    "    if len(uniques)>0:\n",
    "        for class_ in tqdm(uniques, desc='Unique classes atribution'):\n",
    "            index = y_pred[y_pred[f'{levels[i-1]} pred']==class_].index\n",
    "            class_pred = y_test[y_test[:,i-1]==class_][0][i] #Look up for the only class (might be unspecified or not)\n",
    "            y_pred_class = pd.DataFrame([[class_pred, 1]]*len(index), columns=[f'{level} pred', f'{level} prob'], index=index)\n",
    "            new_pred = pd.concat([new_pred, y_pred_class], axis=0)\n",
    "    y_pred = y_pred.join(new_pred)\n",
    "    y_pred[f'{level} prob'] = y_pred.apply(lambda row : row[f'{levels[i-1]} prob']*row[f'{level} prob'], axis=1)\n",
    "y_pred.to_pickle('Results/test_validation/pred_df.pkl') #Results from prediction on the 4 levels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1652118",
   "metadata": {},
   "source": [
    "### Blocking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84ef40a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = pd.read_pickle('Results/test_validation/pred_df.pkl') #Real prediction on test results\n",
    "block_df = pd.DataFrame() \n",
    "for threshold in tqdm([0, 0.5, 0.6, 0.7, 0.8, 0.9, 0.95]):\n",
    "    res_thr = y_pred.copy()\n",
    "    res_thr.loc[res_thr['Kingdom prob']<threshold] = np.nan\n",
    "    res_thr.loc[res_thr['Superclass prob']<threshold, ['Superclass pred', 'Superclass prob', 'Class pred', 'Class prob', 'Subclass pred', 'Subclass prob']] = np.nan\n",
    "    res_thr.loc[res_thr['Class prob']<threshold, ['Class pred', 'Class prob', 'Subclass pred', 'Subclass prob']] = np.nan\n",
    "    res_thr.loc[res_thr['Subclass prob']<threshold, ['Subclass pred', 'Subclass prob']] = np.nan\n",
    "    res_thr['Threshold'] = threshold\n",
    "    res_thr['Number predicted levels'] = res_thr.apply(lambda row: row.iloc[[0, 2, 4, 6]].count(), axis=1)\n",
    "    res_thr = res_thr.reset_index().rename(columns={'index':'sample index'})\n",
    "    block_df = pd.concat([block_df, res_thr])\n",
    "block_df.reset_index(drop=True, inplace=True)\n",
    "#Join true classes to the DF\n",
    "block_df = block_df.join(pd.DataFrame(y_test, columns=['Kingdom', 'Superclass', 'Class', 'Subclass']), on='sample index')\n",
    "block_df.to_pickle('Results/test_validation/pred_w_blocking.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c591e9b2",
   "metadata": {},
   "source": [
    "### Number of compounds by leaf level after blocking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc305782",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "block_df = pd.read_pickle('Results/test_validation/pred_w_blocking.pkl')\n",
    "sns.set_style('white')\n",
    "\n",
    "rcParams['figure.figsize'] = 16, 10\n",
    "rcParams['figure.dpi'] = 800\n",
    "rcParams['axes.titlesize'] = 20\n",
    "rcParams['xtick.labelsize'] = 15\n",
    "rcParams['ytick.labelsize'] = 13\n",
    "rcParams['axes.labelsize'] = 15\n",
    "rcParams['font.size'] = 10\n",
    "\n",
    "ax = sns.countplot(x=\"Number predicted levels\", hue='Threshold', data=block_df[block_df['Threshold']>=0.5], \n",
    "                   palette=sns.color_palette(\"Blues\"), dodge=True)\n",
    "\n",
    "class_order = [0, 1, 2, 3, 4] \n",
    "hue_order = [0.5, 0.6, 0.7, 0.8, 0.9, 0.95]\n",
    "bar_order = product(hue_order, class_order)\n",
    "spots = zip(ax.patches, bar_order)\n",
    "\n",
    "for spot in spots:\n",
    "    total = len(block_df[block_df['Threshold']==spot[1][0]])\n",
    "    level_total = len(block_df[(block_df['Number predicted levels']==spot[1][1]) & \n",
    "        (block_df['Threshold']==spot[1][0])])\n",
    "    height = spot[0].get_height()\n",
    "    if np.isnan(height):\n",
    "        height=0\n",
    "    #ax.text(spot[0].get_x()+0.07, height+2*10**3, '{:1.0f}'.format(level_total), ha='center', va='center')\n",
    "    ax.text(spot[0].get_x()+0.07, height+ 1e3, '{:1.0f}%'.format(level_total*100/total), ha='center', va='center')\n",
    "\n",
    "ax.set_ylabel('Number of samples')\n",
    "ax.set_xlabel('')\n",
    "ax.set_xticklabels(['No prediction', 'Kingdom', 'Kingdom &\\n Superclass', 'Kingdom, Superclass,\\n& Class', \n",
    "                    'Kingdom, Superclass,\\nClass, & Subclass'])\n",
    "ax.set(yscale='linear')\n",
    "ax.yaxis.set_major_formatter(OOMFormatter(3, '%1.0f'))\n",
    "plt.legend(title='Threshold', fontsize='large', loc='upper left')\n",
    "\n",
    "plt.savefig(f'Plots/Test prediction(n of levels).png', bbox_inches='tight', transparent=True, dpi=800)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71b01034",
   "metadata": {},
   "outputs": [],
   "source": [
    "block_df = pd.read_pickle('Results/test_validation/pred_w_blocking.pkl')\n",
    "##Number of compounds with subclass prediction not using an actual classifier for Class and/or Subclass prediction, \n",
    "#from direct interpolation from unique childs\n",
    "for thr in [0, 0.5, 0.6, 0.7, 0.8, 0.9, 0.95]:\n",
    "    print('Threshold - ', thr)\n",
    "    thr_results = block_df[block_df['Threshold']==thr]\n",
    "    thr_results = thr_results[thr_results['Number predicted levels']==4]\n",
    "    a = thr_results.apply(lambda row: 1 if row['Class pred'] in c_u or row['Superclass pred'] in sc_u else 0, axis=1)\n",
    "    print(a.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efd5395b",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Number of compounds with Class leaf prediction not using an actual classifier for Class prediction, \n",
    "#from direct interpolation from parents with unique childs\n",
    "#0 thr not included because prediction will never end on Class level\n",
    "for thr in [0.5, 0.6, 0.7, 0.8, 0.9, 0.95]:\n",
    "    print('Threshold - ', thr)\n",
    "    thr_results = block_df[block_df['Threshold']==thr]\n",
    "    thr_results = thr_results[thr_results['Number predicted levels']==3]\n",
    "    a = thr_results.apply(lambda row: 1 if row['Superclass pred'] in sc_u else 0, axis=1)\n",
    "    print(a.sum())\n",
    "\n",
    "#number increases and then decreases because this is restricted to compounds with a leaf on Class level. Increasing\n",
    "#threshold will make more difficult for compounds to be further predicted and pass 0.7, confidence of prediction of \n",
    "# these compounds is progressively lost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e3ba52f",
   "metadata": {},
   "source": [
    "### Compute scores and classification reports"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a98e642e",
   "metadata": {},
   "source": [
    "when blocking is applied, samples without that level on prediction are filtered out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e7d33b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "block_df = pd.read_pickle('Results/test_validation/pred_w_blocking.pkl')\n",
    "scores = []\n",
    "clf_reports = []\n",
    "total = len(block_df[(block_df['Threshold']==0) & (block_df['Number predicted levels']==4)])\n",
    "for i, level in enumerate(['Kingdom', 'Superclass', 'Class', 'Subclass']):\n",
    "    for thr in tqdm([0, 0.5, 0.6, 0.7, 0.8, 0.9, 0.95], desc=f'Threshold lvl {i+1}'):\n",
    "        results_thr = block_df[block_df['Threshold']==thr]\n",
    "        y_pred = results_thr[results_thr[level + ' pred'].notna()][level + ' pred']\n",
    "        y_true = results_thr[results_thr[level + ' pred'].notna()][level]\n",
    "        clf_report = classification_report(y_true, y_pred, labels=all_categories[level], output_dict=True)\n",
    "        try:\n",
    "            micro = clf_report['accuracy']\n",
    "        except:\n",
    "            micro = clf_report['micro avg']['f1-score']\n",
    "        n = len(block_df[(block_df['Threshold']==thr) & (block_df['Number predicted levels']>=i+1)])\n",
    "        scores.append({'Level': level, 'Threshold': thr,\n",
    "                        'F1 score (micro)': np.round(micro, 4),\n",
    "                        'F1 score (macro)': np.round(clf_report['macro avg']['f1-score'], 4),\n",
    "                      'Compound coverage': f'{n} ({np.round(n*100/total, 1)}%)'})\n",
    "        for key, value in clf_report.items():\n",
    "            if key in ['accuracy', 'macro avg', 'weighted avg', 'micro avg']:\n",
    "                continue\n",
    "            for key2, value2 in value.items():\n",
    "                clf_reports.append({'Level':level, 'Threshold': thr, 'Categorie':key, 'metric_name':key2, 'value':value2})\n",
    "pd.DataFrame(scores).to_pickle('Results/test_validation/scores.pkl')\n",
    "pd.DataFrame(clf_reports).to_pickle('Results/test_validation/classification_report.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59a330cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_scores = pd.read_pickle('Results/test_validation/scores.pkl')\n",
    "test_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0646ec70",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Flatten above df\n",
    "df = {0:{}, 0.5:{}, 0.6:{}, 0.7:{}, 0.8:{}, 0.9:{}, 0.95:{}}\n",
    "for i, row in test_scores.iterrows():\n",
    "    lvl = row['Level']\n",
    "    df[row['Threshold']][f'{lvl} - macro'] =  row['F1 score (macro)']\n",
    "    df[row['Threshold']][f'{lvl} - micro'] =  row['F1 score (micro)']\n",
    "    df[row['Threshold']][f'{lvl} - compound coverage'] = row['Compound coverage']\n",
    "df = pd.DataFrame.from_dict(df, orient='index')\n",
    "df.to_excel('Results/test_validation/scores.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e31ff748",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65fedbec",
   "metadata": {},
   "source": [
    "### Confusion matrixes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56694da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "block_df = pd.read_pickle('Results/test_validation/pred_w_blocking.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdb09919",
   "metadata": {},
   "outputs": [],
   "source": [
    "level = 'Kingdom'\n",
    "results_thr = block_df[block_df['Threshold']==0]\n",
    "y_pred = results_thr[level + ' pred']\n",
    "y_true = results_thr[level]\n",
    "\n",
    "\n",
    "plot_confusion_matrix(y_true, y_pred, all_categories['Kingdom'], annot=True, font_scale=0.5, annot_size = 6, dpi=800, figsize=(2*1.2,2), \n",
    "                      title=f'Kingdom level\\n(no blocking)', title_size=8, ticklabels_size=4, xticklabel_rotation=0, \n",
    "                      yticklabel_rotation=0, label_size=6, x_ha='center', x_va='top', y_ha='right', y_va='center',\n",
    "                      cbar_kws = {\"shrink\":1, \"pad\":0.02},\n",
    "                      save_dir=f'Plots/ConfusionM/Kingdom.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81f69d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "level = 'Kingdom'\n",
    "results_thr = block_df[block_df['Threshold']==0.95]\n",
    "y_pred = results_thr[level + ' pred']\n",
    "y_true = results_thr[level][y_pred.notna()]\n",
    "y_pred = y_pred[y_pred.notna()]\n",
    "\n",
    "\n",
    "plot_confusion_matrix(y_true, y_pred, all_categories['Kingdom'], annot=True, font_scale=0.5, annot_size = 6, dpi=800, figsize=(2*1.2,2), \n",
    "                      title=f'Kingdom level\\n(threshold=0.95)', title_size=8, ticklabels_size=4, xticklabel_rotation=0, \n",
    "                      yticklabel_rotation=0, label_size=6, x_ha='center', x_va='top', y_ha='right', y_va='center',\n",
    "                      cbar_kws = {\"shrink\":1, \"pad\":0.02},\n",
    "                      save_dir=f'Plots/ConfusionM/Kingdom_thr=0.95.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69ab62cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "level = 'Superclass'\n",
    "results_thr = block_df[block_df['Threshold']==0]\n",
    "y_pred = results_thr[level + ' pred']\n",
    "y_true = results_thr[level]\n",
    "\n",
    "\n",
    "plot_confusion_matrix(y_true, y_pred, all_categories['Superclass'], annot=True, font_scale=0.5, annot_size = 3, dpi=800, \n",
    "                      figsize=(7,5), title=f'Superclass level\\n(no blocking)', title_size=8, ticklabels_size=4, \n",
    "                      xticklabel_rotation=90, yticklabel_rotation=0, label_size=4, x_ha='center', x_va='top', y_ha='right', \n",
    "                      y_va='center', cbar_kws = {'shrink':0.5, 'pad':0.01},\n",
    "                      save_dir=f'Plots/ConfusionM/Superclass.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a06be429",
   "metadata": {},
   "outputs": [],
   "source": [
    "level = 'Superclass'\n",
    "results_thr = block_df[block_df['Threshold']==0.95]\n",
    "y_pred = results_thr[level + ' pred']\n",
    "y_true = results_thr[level][y_pred.notna()]\n",
    "y_pred = y_pred[y_pred.notna()]\n",
    "\n",
    "\n",
    "plot_confusion_matrix(y_true, y_pred, all_categories['Superclass'], annot=True, font_scale=0.5, annot_size = 3, dpi=800, \n",
    "                      figsize=(7,5), title=f'Superclass level\\n(threshold=0.95)', title_size=8, ticklabels_size=4, \n",
    "                      xticklabel_rotation=90, yticklabel_rotation=0, label_size=4, x_ha='center', x_va='top', y_ha='right', \n",
    "                      y_va='center', cbar_kws = {'shrink':0.5, 'pad':0.01},\n",
    "                      save_dir=f'Plots/ConfusionM/Superclass_thr=0.95.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3e2085c",
   "metadata": {},
   "outputs": [],
   "source": [
    "level = 'Superclass'\n",
    "results_thr = block_df[block_df['Threshold']==0.5]\n",
    "y_pred = results_thr[level + ' pred']\n",
    "y_true = results_thr[level][y_pred.notna()]\n",
    "y_pred = y_pred[y_pred.notna()]\n",
    "\n",
    "\n",
    "plot_confusion_matrix(y_true, y_pred, all_categories['Superclass'], annot=True, font_scale=0.5, annot_size = 3, dpi=800, figsize=(7,5), \n",
    "                      title=f'Superclass level\\n(threshold=0.5)', title_size=8, ticklabels_size=4, xticklabel_rotation=90, \n",
    "                      yticklabel_rotation=0, label_size=4, x_ha='center', x_va='top', y_ha='right', y_va='center',\n",
    "                      cbar_kws = {'shrink':0.5, 'pad':0.01},\n",
    "                      save_dir=f'Plots/ConfusionM/Superclass_thr=0.5.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd6bc75f",
   "metadata": {},
   "source": [
    "Confusion matrixes for Class and Subclass would be too big"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "071486bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_reports = pickle.load(open('Results/test_validation/classification_report.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "894bcf87",
   "metadata": {},
   "source": [
    "### Class level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2ffb44d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_theme(style='whitegrid')\n",
    "\n",
    "rcParams['figure.figsize'] = 3.5/2.54, 42/2.54\n",
    "rcParams['figure.dpi'] = 800\n",
    "rcParams['axes.titlesize'] = 12\n",
    "rcParams['xtick.labelsize'] = 9\n",
    "rcParams['ytick.labelsize'] = 7\n",
    "rcParams['font.size'] = 7.5\n",
    "\n",
    "for i, classes_parts in enumerate(np.split(classes, [52, 104, 156, 208, 260])):\n",
    "    data = clf_reports[(clf_reports['Threshold']==0) & (clf_reports['Level']=='Class') & \n",
    "                      (clf_reports['metric_name'].isin(['precision', 'recall'])) &\n",
    "                      (clf_reports['Categorie'].isin(classes_parts))]\n",
    "    ax = sns.barplot(x=\"value\", y=\"Categorie\", hue='metric_name', data=data)\n",
    "    ax.set(ylabel=None, xlabel=None)\n",
    "    ax.set_yticklabels(name_2lines(classes_parts, 30), ha='right', va='center')\n",
    "    ax.set_xticks([0, 0.5, 1])\n",
    "    ax.set(xlim=(0, 1))\n",
    "    sns.move_legend(ax, \"lower center\", bbox_to_anchor=(0, 1), ncol=2, title=None, fontsize=9)\n",
    "    plt.savefig(f'Plots/Class_test_classf_report_{i}', bbox_inches='tight', transparent=True, dpi=800)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b100a90",
   "metadata": {},
   "source": [
    "### No blocking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9c09fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a079a86a",
   "metadata": {},
   "source": [
    "#### number of classes without any prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1b0fad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#number of classes without any prediction\n",
    "len(clf_reports[(clf_reports['Threshold']==0) & (clf_reports['Level']=='Class') & \n",
    "                      (clf_reports['metric_name']=='f1-score') &\n",
    "                      (clf_reports['value']==0)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7ed95b1",
   "metadata": {},
   "source": [
    "Of which inorganic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "860b61ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "ino_classes = np.unique(y_test[y_test[:, 0]=='Inorganic compounds'][:, 2])\n",
    "len(ino_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "793fe442",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(clf_reports[(clf_reports['Threshold']==0) & (clf_reports['Level']=='Class') & \n",
    "                      (clf_reports['metric_name']=='f1-score') &\n",
    "                      (clf_reports['value']==0) & (clf_reports['Categorie'].isin(ino_classes))])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e159d7d",
   "metadata": {},
   "source": [
    "#### number of classes with recall lower than 0.2 and different than 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0755efbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(clf_reports[(clf_reports['Threshold']==0) & (clf_reports['Level']=='Class') & \n",
    "                      (clf_reports['metric_name']=='recall') &\n",
    "                      (clf_reports['value']<=0.2)]) - 74"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1edf81fd",
   "metadata": {},
   "source": [
    "of which inorganic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3fc1d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(clf_reports[(clf_reports['Threshold']==0) & (clf_reports['Level']=='Class') & \n",
    "                      (clf_reports['metric_name']=='f1-score') &\n",
    "                      (clf_reports['value']<=0.2) & (clf_reports['Categorie'].isin(ino_classes))]) - 2 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4064f2f0",
   "metadata": {},
   "source": [
    "#### number of classes with recall higher than 0.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48e20485",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(clf_reports[(clf_reports['Threshold']==0) & (clf_reports['Level']=='Class') & \n",
    "                      (clf_reports['metric_name']=='recall') &\n",
    "                      (clf_reports['value']>=0.7)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfa6d784",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(clf_reports[(clf_reports['Threshold']==0) & (clf_reports['Level']=='Class') & \n",
    "                      (clf_reports['metric_name']=='f1-score') &\n",
    "                      (clf_reports['value']>=0.7) & (clf_reports['Categorie'].isin(ino_classes))])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6677e22",
   "metadata": {},
   "source": [
    "### Blocking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b44495f",
   "metadata": {},
   "source": [
    "#### number of classes without any prediction (0.5 thr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "586b7a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#number of classes without any prediction\n",
    "len(clf_reports[(clf_reports['Threshold']==0.5) & (clf_reports['Level']=='Class') & \n",
    "                (clf_reports['metric_name']=='f1-score') &\n",
    "                (clf_reports['value']==0)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6112c97",
   "metadata": {},
   "source": [
    "#### number of classes without any prediction (0.95 thr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb087ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#number of classes without any prediction\n",
    "len(clf_reports[(clf_reports['Threshold']==0.95) & (clf_reports['Level']=='Class') & \n",
    "                (clf_reports['metric_name']=='f1-score') &\n",
    "                (clf_reports['value']==0)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "893c4cad",
   "metadata": {},
   "source": [
    "### Subclass level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e037870",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_classified = clf_reports[(clf_reports['Threshold']==0) & (clf_reports['Level']=='Subclass') & (clf_reports['metric_name'] == 'f1-score') & (clf_reports['value']==0)]['Categorie'].drop_duplicates().values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab2304c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "classified = clf_reports[(clf_reports['Threshold']==0) & (clf_reports['Level']=='Subclass') & (clf_reports['metric_name'] == 'f1-score') & (clf_reports['value']!=0)]['Categorie'].drop_duplicates().values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d34f0190",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(no_classified)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecd53920",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(classified)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f193c0e4",
   "metadata": {},
   "source": [
    "Take out all classes with score==0 to decrease figures size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59c775ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_theme(style='whitegrid')\n",
    "\n",
    "rcParams['figure.figsize'] = 3.5/2.54, 42/2.54\n",
    "rcParams['figure.dpi'] = 800\n",
    "rcParams['axes.titlesize'] = 12\n",
    "rcParams['xtick.labelsize'] = 9\n",
    "rcParams['ytick.labelsize'] = 7\n",
    "rcParams['font.size'] = 7.5\n",
    "\n",
    "for i, classes_parts in enumerate(np.split(classified, [59, 118, 177, 236, 295, 354, 413, 472])):\n",
    "    data = clf_reports[(clf_reports['Threshold']==0) & (clf_reports['Level']=='Subclass') & \n",
    "                      (clf_reports['metric_name'].isin(['precision', 'recall'])) &\n",
    "                      (clf_reports['Categorie'].isin(classes_parts))]\n",
    "    ax = sns.barplot(x=\"value\", y=\"Categorie\", hue='metric_name', data=data)\n",
    "    ax.set(ylabel=None, xlabel=None)\n",
    "    ax.set_yticklabels(name_2lines(classes_parts, 30), ha='right', va='center')\n",
    "    ax.set_xticks([0, 0.5, 1])\n",
    "    ax.set(xlim=(0, 1))\n",
    "    sns.move_legend(ax, \"lower center\", bbox_to_anchor=(0, 1), ncol=2, title=None, fontsize=9)\n",
    "    plt.savefig(f'Plots/Subclass_test_classf_report_{i}', bbox_inches='tight', transparent=True, dpi=800)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3b91700",
   "metadata": {},
   "source": [
    "#### Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa546dfe",
   "metadata": {},
   "source": [
    "##### No blocking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5ca4b8e",
   "metadata": {},
   "source": [
    "#### number of classes without any prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aff598f",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(subclasses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5128975",
   "metadata": {},
   "outputs": [],
   "source": [
    "#number of classes without any prediction\n",
    "len(clf_reports[(clf_reports['Threshold']==0) & (clf_reports['Level']=='Subclass') & \n",
    "                      (clf_reports['metric_name']=='f1-score') &\n",
    "                      (clf_reports['value']==0)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7593b7d2",
   "metadata": {},
   "source": [
    "Of which inorganic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80f99195",
   "metadata": {},
   "outputs": [],
   "source": [
    "ino_subclasses = np.unique(y_test[y_test[:, 0]=='Inorganic compounds'][:, 3])\n",
    "len(ino_subclasses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "628504d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(clf_reports[(clf_reports['Threshold']==0) & (clf_reports['Level']=='Subclass') & \n",
    "                      (clf_reports['metric_name']=='f1-score') &\n",
    "                      (clf_reports['value']==0) & (clf_reports['Categorie'].isin(ino_subclasses))])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44f5c46d",
   "metadata": {},
   "source": [
    "#### number of classes with recall lower than 0.2 and different than 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef7babda",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(clf_reports[(clf_reports['Threshold']==0) & (clf_reports['Level']=='Subclass') & \n",
    "                      (clf_reports['metric_name']=='recall') &\n",
    "                      (clf_reports['value']<=0.2)]) - 192"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3732d90",
   "metadata": {},
   "source": [
    "of which inorganic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aa626cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(clf_reports[(clf_reports['Threshold']==0) & (clf_reports['Level']=='Subclass') & \n",
    "                      (clf_reports['metric_name']=='f1-score') &\n",
    "                      (clf_reports['value']<=0.2) & (clf_reports['Categorie'].isin(ino_subclasses))]) - 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36df7c00",
   "metadata": {},
   "source": [
    "#### number of classes with recall higher than 0.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7e326ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(clf_reports[(clf_reports['Threshold']==0) & (clf_reports['Level']=='Subclass') & \n",
    "                      (clf_reports['metric_name']=='recall') &\n",
    "                      (clf_reports['value']>=0.7)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef41d14a",
   "metadata": {},
   "source": [
    "of which inorganic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49ab159e",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(clf_reports[(clf_reports['Threshold']==0) & (clf_reports['Level']=='Subclass') & \n",
    "                      (clf_reports['metric_name']=='f1-score') &\n",
    "                      (clf_reports['value']>=0.7) & (clf_reports['Categorie'].isin(ino_subclasses))])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34b337e6",
   "metadata": {},
   "source": [
    "### Blocking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1fb858f",
   "metadata": {},
   "source": [
    "#### number of classes without any prediction (0.5 thr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7773cb07",
   "metadata": {},
   "outputs": [],
   "source": [
    "#number of classes without any prediction\n",
    "len(clf_reports[(clf_reports['Threshold']==0.5) & (clf_reports['Level']=='Subclass') & \n",
    "                (clf_reports['metric_name']=='f1-score') &\n",
    "                (clf_reports['value']==0)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfc7ff4b",
   "metadata": {},
   "source": [
    "#### number of classes without any prediction (0.95 thr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6c5cbf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#number of classes without any prediction\n",
    "len(clf_reports[(clf_reports['Threshold']==0.95) & (clf_reports['Level']=='Subclass') & \n",
    "                (clf_reports['metric_name']=='f1-score') &\n",
    "                (clf_reports['value']==0)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7a342e7",
   "metadata": {},
   "source": [
    "## Biological Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9f19c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "db = pd.read_pickle('../Dataset_Preprocessed.pkl')\n",
    "yeast_data = pd.read_excel('Results/bio_validation/Yeast MS data.xlsx')\n",
    "fing_data = pd.read_excel('Results/bio_validation/Fingerprint MS data.xlsx')\n",
    "scaler = pickle.load(open('Dataset/fitted_scaler.pkl', 'rb'))\n",
    "yeast_data['Bucket label'] = yeast_data['Bucket label'].str.split(expand=True)[0].str.replace(',', '.').astype(float)\n",
    "yeast_data = yeast_data.set_index('Bucket label')\n",
    "fing_data['Bucket label'] = fing_data['Bucket label'].str.split(expand=True)[0].str.replace(',', '.').astype(float)\n",
    "fing_data = fing_data.set_index('Bucket label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e56a3c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "yeast_df = {}\n",
    "#Join samples, keep peak it it is present in at least 2 of the replicates\n",
    "for sample in ['BY0', 'GRE3', 'ENO1', 'GLO1', 'GLO2']:\n",
    "    yeast_df[sample] = yeast_data.loc[:, yeast_data.columns[yeast_data.columns.str.startswith(sample)]].apply(lambda x: len(x[x==0])<2, axis=1)\n",
    "yeast_df = pd.DataFrame(yeast_df)\n",
    "yeast_df = yeast_df[yeast_df.any(axis=1)]\n",
    "yeast_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d06abe21",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_matches = []\n",
    "for peak in tqdm(yeast_df.index, total=len(yeast_df)):\n",
    "    matches = []\n",
    "    ppm_dev = abs((db['Mass']-peak)/peak)*10**6\n",
    "    ppm_dev_below_1 = ppm_dev[ppm_dev<1]\n",
    "    min_ = ppm_dev_below_1.min()\n",
    "    if not np.isnan(min_):\n",
    "        ppm_dev_min = ppm_dev[ppm_dev==min_].index\n",
    "        matches = []\n",
    "        features = db.loc[ppm_dev_min[0], :].iloc[6:]\n",
    "        yeast_df.loc[peak, features.index] = features.values\n",
    "        for i, row in db.loc[ppm_dev_min, ['Kingdom', 'Superclass', 'Class', 'Subclass']].drop_duplicates().iterrows():\n",
    "            matches.append([row['Kingdom'], row['Superclass'], row['Class'], row['Subclass']])\n",
    "        all_matches.append(matches)\n",
    "    else:\n",
    "        all_matches.append(np.nan)\n",
    "yeast_df.loc[:, 'Matches'] = pd.Series(all_matches, index=yeast_df.index)\n",
    "yeast_df.insert(5, 'Matches', yeast_df.pop('Matches'))\n",
    "yeast_df = yeast_df.dropna()\n",
    "yeast_df.to_pickle('Results/bio_validation/yeast_annotation.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b33f4b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "fing_df = {}\n",
    "#Join samples, keep peak it it is present in at least 2 of the replicates\n",
    "for sample in ['V1', 'V2', 'V3', 'V4', 'V5', 'V6']:\n",
    "    fing_df[sample] = fing_data.loc[:, fing_data.columns[fing_data.columns.str.startswith(sample)]].apply(lambda x: len(x[x==0])<2, axis=1)\n",
    "fing_df = pd.DataFrame(fing_df)\n",
    "fing_df = fing_df[fing_df.any(axis=1)]\n",
    "fing_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cc6f16b",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_matches = []\n",
    "for peak in tqdm(fing_df.index, total=len(fing_df)):\n",
    "    matches = []\n",
    "    ppm_dev = abs((db['Mass']-peak)/peak)*10**6\n",
    "    ppm_dev_below_1 = ppm_dev[ppm_dev<1]\n",
    "    min_ = ppm_dev_below_1.min()\n",
    "    if not np.isnan(min_):\n",
    "        ppm_dev_min = ppm_dev[ppm_dev==min_].index\n",
    "        matches = []\n",
    "        features = db.loc[ppm_dev_min[0], :].iloc[6:]\n",
    "        fing_df.loc[peak, features.index] = features.values\n",
    "        for i, row in db.loc[ppm_dev_min, ['Kingdom', 'Superclass', 'Class', 'Subclass']].drop_duplicates().iterrows():\n",
    "            matches.append([row['Kingdom'], row['Superclass'], row['Class'], row['Subclass']])\n",
    "        all_matches.append(matches)\n",
    "    else:\n",
    "        all_matches.append(np.nan)\n",
    "fing_df.loc[:, 'Matches'] = pd.Series(all_matches, index=fing_df.index)\n",
    "fing_df.insert(6, 'Matches', fing_df.pop('Matches'))\n",
    "fing_df = fing_df.dropna()\n",
    "fing_df.to_pickle('Results/bio_validation/fing_annotation.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f978eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "yeast_df = pd.read_pickle('Results/bio_validation/yeast_annotation.pkl')\n",
    "fing_df = pd.read_pickle('Results/bio_validation/fing_annotation.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb218a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for sample in ['BY0', 'GRE3', 'ENO1', 'GLO1', 'GLO2']:\n",
    "    print(len(yeast_df[yeast_df[sample]==True]))\n",
    "print(len(yeast_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d410ae66",
   "metadata": {},
   "outputs": [],
   "source": [
    "for sample in ['V1', 'V2', 'V3', 'V4', 'V5', 'V6']:\n",
    "    print(len(fing_df[fing_df[sample]==True]))\n",
    "print(len(fing_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87dd0b95",
   "metadata": {},
   "source": [
    "### Make prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a5555b2",
   "metadata": {},
   "source": [
    "### Load the classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab9aeba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfs = {}\n",
    "for level in ['Kingdom', 'Superclass', 'Class', 'Subclass']:\n",
    "    print(level)\n",
    "    cls = hierarchy[level]['classes']\n",
    "    uniques = hierarchy[level]['uniques']\n",
    "    cfs[level] = {}\n",
    "    for i, class_ in tqdm(enumerate(cls), total=len(cls)):\n",
    "        if class_ in uniques:\n",
    "            continue\n",
    "        elif level == 'Kingdom':\n",
    "            i = ''\n",
    "        else:\n",
    "            i = f'_{i}'\n",
    "        with open(f'Selected_classifiers/{level}{i}.pkl', 'rb') as f:\n",
    "            cf = pickle.load(f)\n",
    "            if cf.n_features_in_ != 133:\n",
    "                if level == 'Kingdom':\n",
    "                    sel_f = pickle.load(open('Results/selected_features.pkl', 'rb'))[level]['selected features']\n",
    "                else:\n",
    "                    sel_f = pickle.load(open('Results/selected_features.pkl', 'rb'))[level][class_]['selected features']\n",
    "                f_index = [feature_names.index(feature) for feature in sel_f]\n",
    "            else:\n",
    "                f_index = list(range(len(feature_names)))\n",
    "            cfs[level][class_] = (cf, f_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13bd65e4",
   "metadata": {},
   "source": [
    "### Predict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "128d1abc",
   "metadata": {},
   "source": [
    "##### Yeast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51cf0eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle('Results/bio_validation/yeast_annotation.pkl')\n",
    "X = df.iloc[:, 6:].values\n",
    "X_scal = scaler.transform(X)\n",
    "\n",
    "levels = ['Kingdom', 'Superclass', 'Class', 'Subclass']\n",
    "\n",
    "#Kingdom prediction\n",
    "cf, fi = cfs['Kingdom']['Chemical entities']\n",
    "#first level prediction is joined into a df\n",
    "#decision of scaling or not is done inside this function (based if it is RF or not)\n",
    "y_pred = get_class_w_prob(cf, 'Kingdom', X[:, fi], X_scal[:, fi])\n",
    "\n",
    "#Sublevels prediction\n",
    "for i, level in enumerate(levels):\n",
    "    if i==0:\n",
    "        continue\n",
    "    print(level)\n",
    "    #classes that do not need classifier\n",
    "    uniques = hierarchy[level]['uniques']\n",
    "    #df for the prediciton of the whole level\n",
    "    new_pred = pd.DataFrame()\n",
    "    level_cfs = cfs[level]\n",
    "    #iterate each parent node to make predictions on the whole level\n",
    "    for class_ in tqdm(level_cfs.keys(), desc='Classifiers prediction'):\n",
    "        cf, fi = level_cfs[class_]\n",
    "        #index of the samples to be classified (the ones with the parent node predicted at previous level)\n",
    "        index = y_pred[y_pred[f'{levels[i-1]} pred']==class_].index\n",
    "        if np.shape(X_test[index])[0] == 0: #There is nothing to predict\n",
    "            continue\n",
    "        y_pred_class = get_class_w_prob(cf, level, X[index][:, fi], X_scal[index][:, fi], index=index)\n",
    "        new_pred = pd.concat([new_pred, y_pred_class], axis=0)\n",
    "    if len(uniques)>0:\n",
    "        for class_ in tqdm(uniques, desc='Unique classes atribution'):\n",
    "            index = y_pred[y_pred[f'{levels[i-1]} pred']==class_].index\n",
    "            class_pred = y_test[y_test[:,i-1]==class_][0][i] #Look up for the only class (might be unspecified or not)\n",
    "            y_pred_class = pd.DataFrame([[class_pred, 1]]*len(index), columns=[f'{level} pred', f'{level} prob'], index=index)\n",
    "            new_pred = pd.concat([new_pred, y_pred_class], axis=0)\n",
    "    y_pred = y_pred.join(new_pred)\n",
    "    y_pred[f'{level} prob'] = y_pred.apply(lambda row : row[f'{levels[i-1]} prob']*row[f'{level} prob'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "166f16b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Join pred to original DF\n",
    "df.loc[:, y_pred.columns] = y_pred.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ee6b0d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,row in df.iterrows():\n",
    "    pred = row[['Kingdom pred', 'Superclass pred', 'Class pred', 'Subclass pred']].values\n",
    "    matches = np.array(row['Matches'])\n",
    "    if np.shape(matches)[0] == 1:\n",
    "        df.loc[row.name, ['Kingdom', 'Superclass', 'Class', 'Subclass']] = matches[0]\n",
    "        continue\n",
    "    for match in matches:\n",
    "        if np.array_equal(match,pred): #if there is an exact match\n",
    "            df.loc[row.name, ['Kingdom', 'Superclass', 'Class', 'Subclass']] = match\n",
    "            continue\n",
    "    for i in [2, 1, 0]:\n",
    "        if np.any(matches[:, i] == pred[i]):\n",
    "            df.loc[row.name, ['Kingdom', 'Superclass', 'Class', 'Subclass']] = matches[matches[:, i]==pred[i]][0]\n",
    "            break\n",
    "    continue\n",
    "print(df[['Kingdom', 'Superclass', 'Class', 'Subclass']].info())\n",
    "df = df[['BY0', 'GRE3', 'ENO1', 'GLO1', 'GLO2', 'Kingdom', 'Kingdom pred', 'Kingdom prob', \n",
    "                 'Superclass', 'Superclass pred', 'Superclass prob', 'Class', 'Class pred', 'Class prob', \n",
    "                 'Subclass', 'Subclass pred', 'Subclass prob']]\n",
    "df = df.reset_index()\n",
    "df.to_pickle('Results/bio_validation/yeast_pred_df.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33b59184",
   "metadata": {},
   "source": [
    "##### Fingerprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "440dbd5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle('Results/bio_validation/fing_annotation.pkl')\n",
    "X = df.iloc[:, 7:].values\n",
    "X_scal = scaler.transform(X)\n",
    "\n",
    "levels = ['Kingdom', 'Superclass', 'Class', 'Subclass']\n",
    "\n",
    "#Kingdom prediction\n",
    "cf, fi = cfs['Kingdom']['Chemical entities']\n",
    "#first level prediction is joined into a df\n",
    "#decision of scaling or not is done inside this function (based if it is RF or not)\n",
    "y_pred = get_class_w_prob(cf, 'Kingdom', X[:, fi], X_scal[:, fi])\n",
    "\n",
    "#Sublevels prediction\n",
    "for i, level in enumerate(levels):\n",
    "    if i==0:\n",
    "        continue\n",
    "    print(level)\n",
    "    #classes that do not need classifier\n",
    "    uniques = hierarchy[level]['uniques']\n",
    "    #df for the prediciton of the whole level\n",
    "    new_pred = pd.DataFrame()\n",
    "    level_cfs = cfs[level]\n",
    "    #iterate each parent node to make predictions on the whole level\n",
    "    for class_ in tqdm(level_cfs.keys(), desc='Classifiers prediction'):\n",
    "        cf, fi = level_cfs[class_]\n",
    "        #index of the samples to be classified (the ones with the parent node predicted at previous level)\n",
    "        index = y_pred[y_pred[f'{levels[i-1]} pred']==class_].index\n",
    "        if np.shape(X_test[index])[0] == 0: #There is nothing to predict\n",
    "            continue\n",
    "        y_pred_class = get_class_w_prob(cf, level, X[index][:, fi], X_scal[index][:, fi], index=index)\n",
    "        new_pred = pd.concat([new_pred, y_pred_class], axis=0)\n",
    "    if len(uniques)>0:\n",
    "        for class_ in tqdm(uniques, desc='Unique classes atribution'):\n",
    "            index = y_pred[y_pred[f'{levels[i-1]} pred']==class_].index\n",
    "            class_pred = y_test[y_test[:,i-1]==class_][0][i] #Look up for the only class (might be unspecified or not)\n",
    "            y_pred_class = pd.DataFrame([[class_pred, 1]]*len(index), columns=[f'{level} pred', f'{level} prob'], index=index)\n",
    "            new_pred = pd.concat([new_pred, y_pred_class], axis=0)\n",
    "    y_pred = y_pred.join(new_pred)\n",
    "    y_pred[f'{level} prob'] = y_pred.apply(lambda row : row[f'{levels[i-1]} prob']*row[f'{level} prob'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28a84282",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Join pred to original DF\n",
    "df.loc[:, y_pred.columns] = y_pred.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e68f5116",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,row in df.iterrows():\n",
    "    pred = row[['Kingdom pred', 'Superclass pred', 'Class pred', 'Subclass pred']].values\n",
    "    matches = np.array(row['Matches'])\n",
    "    if np.shape(matches)[0] == 1:\n",
    "        df.loc[row.name, ['Kingdom', 'Superclass', 'Class', 'Subclass']] = matches[0]\n",
    "        continue\n",
    "    for match in matches:\n",
    "        if np.array_equal(match,pred): #if there is an exact match\n",
    "            df.loc[row.name, ['Kingdom', 'Superclass', 'Class', 'Subclass']] = match\n",
    "            continue\n",
    "    for i in [2, 1, 0]:\n",
    "        if np.any(matches[:, i] == pred[i]):\n",
    "            df.loc[row.name, ['Kingdom', 'Superclass', 'Class', 'Subclass']] = matches[matches[:, i]==pred[i]][0]\n",
    "            break\n",
    "    continue\n",
    "print(df[['Kingdom', 'Superclass', 'Class', 'Subclass']].info())\n",
    "df = df[['V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'Kingdom', 'Kingdom pred', 'Kingdom prob', \n",
    "                 'Superclass', 'Superclass pred', 'Superclass prob', 'Class', 'Class pred', 'Class prob', \n",
    "                 'Subclass', 'Subclass pred', 'Subclass prob']]\n",
    "df = df.reset_index()\n",
    "df.to_pickle('Results/bio_validation/fing_pred_df.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d96e79ce",
   "metadata": {},
   "source": [
    "### Blocking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6c2c691",
   "metadata": {},
   "source": [
    "#### Yeast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd06b4bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = pd.read_pickle('Results/bio_validation/yeast_pred_df.pkl') #Real prediction on test results\n",
    "block_df = pd.DataFrame() \n",
    "for threshold in tqdm([0, 0.5, 0.6, 0.7, 0.8, 0.9, 0.95]):\n",
    "    res_thr = y_pred.copy()\n",
    "    res_thr.loc[res_thr['Kingdom prob']<threshold] = np.nan\n",
    "    res_thr.loc[res_thr['Superclass prob']<threshold, ['Superclass pred', 'Superclass prob', 'Class pred', 'Class prob', 'Subclass pred', 'Subclass prob']] = np.nan\n",
    "    res_thr.loc[res_thr['Class prob']<threshold, ['Class pred', 'Class prob', 'Subclass pred', 'Subclass prob']] = np.nan\n",
    "    res_thr.loc[res_thr['Subclass prob']<threshold, ['Subclass pred', 'Subclass prob']] = np.nan\n",
    "    res_thr['Threshold'] = threshold\n",
    "    res_thr['Number predicted levels'] = res_thr.apply(lambda row: row.iloc[[7, 10, 13, 16]].count(), axis=1)\n",
    "    res_thr = res_thr.reset_index().rename(columns={'index':'sample index'})\n",
    "    block_df = pd.concat([block_df, res_thr])\n",
    "block_df.reset_index(drop=True, inplace=True)\n",
    "#Join true classes to the DF\n",
    "block_df.to_pickle('Results/bio_validation/yeast_pred_w_blocking.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f25816ee",
   "metadata": {},
   "source": [
    "#### Fingerprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40dffd96",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = pd.read_pickle('Results/bio_validation/fing_pred_df.pkl') #Real prediction on test results\n",
    "block_df = pd.DataFrame() \n",
    "for threshold in tqdm([0, 0.5, 0.6, 0.7, 0.8, 0.9, 0.95]):\n",
    "    res_thr = y_pred.copy()\n",
    "    res_thr.loc[res_thr['Kingdom prob']<threshold] = np.nan\n",
    "    res_thr.loc[res_thr['Superclass prob']<threshold, ['Superclass pred', 'Superclass prob', 'Class pred', 'Class prob', 'Subclass pred', 'Subclass prob']] = np.nan\n",
    "    res_thr.loc[res_thr['Class prob']<threshold, ['Class pred', 'Class prob', 'Subclass pred', 'Subclass prob']] = np.nan\n",
    "    res_thr.loc[res_thr['Subclass prob']<threshold, ['Subclass pred', 'Subclass prob']] = np.nan\n",
    "    res_thr['Threshold'] = threshold\n",
    "    res_thr['Number predicted levels'] = res_thr.apply(lambda row: row.iloc[[8, 11, 14, 17]].count(), axis=1)\n",
    "    res_thr = res_thr.reset_index().rename(columns={'index':'sample index'})\n",
    "    block_df = pd.concat([block_df, res_thr])\n",
    "block_df.reset_index(drop=True, inplace=True)\n",
    "#Join true classes to the DF\n",
    "block_df.to_pickle('Results/bio_validation/fing_pred_w_blocking.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7df0ef82",
   "metadata": {},
   "source": [
    "### Compute scores and classification reports"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b7541fc",
   "metadata": {},
   "source": [
    "when blocking is applied, samples without that level on prediction are filtered out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cf4733c",
   "metadata": {},
   "source": [
    "##### Yeast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5303e11e",
   "metadata": {},
   "outputs": [],
   "source": [
    "block_df = pd.read_pickle('Results/bio_validation/yeast_pred_w_blocking.pkl')\n",
    "scores = []\n",
    "clf_reports = []\n",
    "total = len(block_df[(block_df['Threshold']==0) & (block_df['Number predicted levels']==4)])\n",
    "for i, level in enumerate(['Kingdom', 'Superclass', 'Class', 'Subclass']):\n",
    "    for thr in tqdm([0, 0.5, 0.6, 0.7, 0.8, 0.9, 0.95], desc=f'Threshold lvl {i+1}'):\n",
    "        results_thr = block_df[block_df['Threshold']==thr]\n",
    "        y_pred = results_thr[results_thr[level + ' pred'].notna()][level + ' pred']\n",
    "        y_true = results_thr[results_thr[level + ' pred'].notna()][level]\n",
    "        clf_report = classification_report(y_true, y_pred, labels=all_categories[level], output_dict=True)\n",
    "        try:\n",
    "            micro = clf_report['accuracy']\n",
    "        except:\n",
    "            micro = clf_report['micro avg']['f1-score']\n",
    "        n = len(block_df[(block_df['Threshold']==thr) & (block_df['Number predicted levels']>=i+1)])\n",
    "        scores.append({'Level': level, 'Threshold': thr,\n",
    "                        'F1 score (micro)': np.round(micro, 4),\n",
    "                        'F1 score (macro)': np.round(clf_report['macro avg']['f1-score'], 4),\n",
    "                      'Compound coverage': f'{n} ({np.round(n*100/total, 1)}%)'})\n",
    "        for key, value in clf_report.items():\n",
    "            if key in ['accuracy', 'macro avg', 'weighted avg', 'micro avg']:\n",
    "                continue\n",
    "            for key2, value2 in value.items():\n",
    "                clf_reports.append({'Level':level, 'Threshold': thr, 'Categorie':key, 'metric_name':key2, 'value':value2})\n",
    "pd.DataFrame(scores).to_pickle('Results/bio_validation/yeast_scores.pkl')\n",
    "pd.DataFrame(clf_reports).to_pickle('Results/bio_validation/yeast_classification_report.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab76c442",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Flatten above df\n",
    "test_scores = pd.read_pickle('Results/bio_validation/yeast_scores.pkl')\n",
    "df = {0:{}, 0.5:{}, 0.6:{}, 0.7:{}, 0.8:{}, 0.9:{}, 0.95:{}}\n",
    "for i, row in test_scores.iterrows():\n",
    "    lvl = row['Level']\n",
    "    df[row['Threshold']][f'{lvl} - macro'] =  row['F1 score (macro)']\n",
    "    df[row['Threshold']][f'{lvl} - micro'] =  row['F1 score (micro)']\n",
    "    df[row['Threshold']][f'{lvl} - compound coverage'] = row['Compound coverage']\n",
    "df = pd.DataFrame.from_dict(df, orient='index')\n",
    "df.to_excel('Results/bio_validation/yeast_scores.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c412d124",
   "metadata": {},
   "outputs": [],
   "source": [
    "block_df = pd.read_pickle('Results/bio_validation/yeast_pred_w_blocking.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "329e2ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "block_df[(block_df['Threshold']==0)]['Superclass'].value_counts()*100/403"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acc42328",
   "metadata": {},
   "outputs": [],
   "source": [
    "true = block_df[(block_df['Threshold']==0)]['Class']\n",
    "pred = block_df[(block_df['Threshold']==0)]['Class pred']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "842549e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(pd.concat([true, pred]), columns=['Category'])\n",
    "df['Set'] = pd.concat([pd.Series(['True']*403),pd.Series(['Pred']*403)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94dfce90",
   "metadata": {},
   "outputs": [],
   "source": [
    "h = pd.DataFrame(y_train, columns=['Kingdom', 'Superclass', 'Class', 'Subclass'])[['Superclass', 'Class']].sort_values(by=['Superclass', 'Class']).drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ee1f777",
   "metadata": {},
   "outputs": [],
   "source": [
    "h = h[h['Class'].isin(np.unique(df['Category']))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d270bb1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "h1 = h[:34]\n",
    "h2 = h[34:]\n",
    "class_order_1 = h1['Class'].drop_duplicates().values\n",
    "class_order_2 = h2['Class'].drop_duplicates().values\n",
    "super_size_1 = h1['Superclass'].value_counts().loc[h1['Superclass'].drop_duplicates().values]\n",
    "super_size_2 = h2['Superclass'].value_counts().loc[h2['Superclass'].drop_duplicates().values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40729ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "rcParams['figure.figsize'] = 10, 20\n",
    "rcParams['figure.dpi'] = 800\n",
    "rcParams['axes.titlesize'] = 12\n",
    "rcParams['xtick.labelsize'] = 12\n",
    "rcParams['ytick.labelsize'] = 14\n",
    "rcParams['font.size'] = 7.5\n",
    "\n",
    "sns.set_theme(style='whitegrid')\n",
    "\n",
    "ax = sns.countplot(data=df[df['Category'].isin(class_order_1)], y='Category', hue='Set', order=class_order_1, orient='v')\n",
    "ax.set_yticklabels(name_2lines(class_order_1, 39), ha='right', va='center', rotation=0, fontsize=14)\n",
    "ax.set_ylabel(None)\n",
    "ax.set_xlabel(None)\n",
    "\n",
    "before=0\n",
    "for y, size in super_size_1.iteritems():\n",
    "    plt.text(-40, before-0.5 + size/2, name_2lines(y), ha='right', va='center', fontsize=14, weight='bold')\n",
    "    before += size\n",
    "plt.xlim(0, 90)\n",
    "plt.legend(fontsize='x-large', loc='lower right')\n",
    "plt.savefig(f'Plots/yeast_validation_1.png', bbox_inches='tight', transparent=True, dpi=800)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ef36c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "rcParams['figure.figsize'] = 10, 20\n",
    "rcParams['figure.dpi'] = 800\n",
    "rcParams['axes.titlesize'] = 12\n",
    "rcParams['xtick.labelsize'] = 10\n",
    "rcParams['ytick.labelsize'] = 14\n",
    "rcParams['font.size'] = 7.5\n",
    "\n",
    "sns.set_theme(style='whitegrid')\n",
    "\n",
    "ax = sns.countplot(data=df[df['Category'].isin(class_order_2)], y='Category', hue='Set', order=class_order_2, orient='v')\n",
    "ax.set_yticklabels(name_2lines(class_order_2, 40), ha='right', va='center', rotation=0, fontsize=14)\n",
    "ax.set_ylabel(None)\n",
    "ax.set_xlabel(None)\n",
    "\n",
    "before=0\n",
    "for y, size in super_size_2.iteritems():\n",
    "    plt.text(-40, before-0.5 + size/2, name_2lines(y), ha='right', va='center', fontsize=14, weight='bold')\n",
    "    before += size\n",
    "plt.xlim(0, 90)\n",
    "plt.legend(fontsize='x-large', loc='lower right')\n",
    "plt.savefig(f'Plots/yeast_validation_2.png', bbox_inches='tight', transparent=True, dpi=800)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "053d4197",
   "metadata": {},
   "source": [
    "##### Fingerprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcf480c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "block_df = pd.read_pickle('Results/bio_validation/fing_pred_w_blocking.pkl')\n",
    "scores = []\n",
    "clf_reports = []\n",
    "total = len(block_df[(block_df['Threshold']==0) & (block_df['Number predicted levels']==4)])\n",
    "for i, level in enumerate(['Kingdom', 'Superclass', 'Class', 'Subclass']):\n",
    "    for thr in tqdm([0, 0.5, 0.6, 0.7, 0.8, 0.9, 0.95], desc=f'Threshold lvl {i+1}'):\n",
    "        results_thr = block_df[block_df['Threshold']==thr]\n",
    "        y_pred = results_thr[results_thr[level + ' pred'].notna()][level + ' pred']\n",
    "        y_true = results_thr[results_thr[level + ' pred'].notna()][level]\n",
    "        clf_report = classification_report(y_true, y_pred, labels=all_categories[level], output_dict=True)\n",
    "        try:\n",
    "            micro = clf_report['accuracy']\n",
    "        except:\n",
    "            micro = clf_report['micro avg']['f1-score']\n",
    "        n = len(block_df[(block_df['Threshold']==thr) & (block_df['Number predicted levels']>=i+1)])\n",
    "        scores.append({'Level': level, 'Threshold': thr,\n",
    "                        'F1 score (micro)': np.round(micro, 4),\n",
    "                        'F1 score (macro)': np.round(clf_report['macro avg']['f1-score'], 4),\n",
    "                      'Compound coverage': f'{n} ({np.round(n*100/total, 1)}%)'})\n",
    "        for key, value in clf_report.items():\n",
    "            if key in ['accuracy', 'macro avg', 'weighted avg', 'micro avg']:\n",
    "                continue\n",
    "            for key2, value2 in value.items():\n",
    "                clf_reports.append({'Level':level, 'Threshold': thr, 'Categorie':key, 'metric_name':key2, 'value':value2})\n",
    "pd.DataFrame(scores).to_pickle('Results/bio_validation/fing_scores.pkl')\n",
    "pd.DataFrame(clf_reports).to_pickle('Results/bio_validation/fing_classification_report.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39162aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Flatten above df\n",
    "test_scores = pd.read_pickle('Results/bio_validation/fing_scores.pkl')\n",
    "df = {0:{}, 0.5:{}, 0.6:{}, 0.7:{}, 0.8:{}, 0.9:{}, 0.95:{}}\n",
    "for i, row in test_scores.iterrows():\n",
    "    lvl = row['Level']\n",
    "    df[row['Threshold']][f'{lvl} - macro'] =  row['F1 score (macro)']\n",
    "    df[row['Threshold']][f'{lvl} - micro'] =  row['F1 score (micro)']\n",
    "    df[row['Threshold']][f'{lvl} - compound coverage'] = row['Compound coverage']\n",
    "df = pd.DataFrame.from_dict(df, orient='index')\n",
    "df.to_excel('Results/bio_validation/fing_scores.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "851385fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "block_df = pd.read_pickle('Results/bio_validation/fing_pred_w_blocking.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46bccab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "block_df[(block_df['Threshold']==0)]['Superclass'].value_counts()*100/311"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a936067",
   "metadata": {},
   "outputs": [],
   "source": [
    "true = block_df[(block_df['Threshold']==0)]['Class']\n",
    "pred = block_df[(block_df['Threshold']==0)]['Class pred']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bab24a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(pd.concat([true, pred]), columns=['Category'])\n",
    "df['Set'] = pd.concat([pd.Series(['True']*311),pd.Series(['Pred']*311)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cfa041b",
   "metadata": {},
   "outputs": [],
   "source": [
    "h = a[['Superclass', 'Class']].sort_values(by=['Superclass', 'Class']).drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ee7da80",
   "metadata": {},
   "outputs": [],
   "source": [
    "h = h[h['Class'].isin(np.unique(df['Category']))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7bfce33",
   "metadata": {},
   "outputs": [],
   "source": [
    "h1 = h[:16]\n",
    "h2 = h[16:]\n",
    "class_order_1 = h1['Class'].drop_duplicates().values\n",
    "class_order_2 = h2['Class'].drop_duplicates().values\n",
    "super_size_1 = h1['Superclass'].value_counts().loc[h1['Superclass'].drop_duplicates().values]\n",
    "super_size_2 = h2['Superclass'].value_counts().loc[h2['Superclass'].drop_duplicates().values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c481a7db",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "rcParams['figure.figsize'] = 10, 20\n",
    "rcParams['figure.dpi'] = 800\n",
    "rcParams['axes.titlesize'] = 12\n",
    "rcParams['xtick.labelsize'] = 12\n",
    "rcParams['ytick.labelsize'] = 14\n",
    "rcParams['font.size'] = 7.5\n",
    "\n",
    "sns.set_theme(style='whitegrid')\n",
    "\n",
    "ax = sns.countplot(data=df[df['Category'].isin(class_order_1)], y='Category', hue='Set', order=class_order_1, orient='v')\n",
    "ax.set_yticklabels(name_2lines(class_order_1, 30), ha='right', va='center', rotation=0, fontsize=14)\n",
    "ax.set_ylabel(None)\n",
    "ax.set_xlabel(None)\n",
    "\n",
    "before=0\n",
    "for y, size in super_size_1.iteritems():\n",
    "    plt.text(-25, before-0.5 + size/2, name_2lines(y), ha='right', va='center', fontsize=14, weight='bold')\n",
    "    before += size\n",
    "plt.xlim(0, 60)\n",
    "plt.legend(fontsize='x-large', loc='lower right')\n",
    "plt.savefig(f'Plots/fing_validation_1.png', bbox_inches='tight', transparent=True, dpi=800)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fce8ebc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "rcParams['figure.figsize'] = 10, 20\n",
    "rcParams['figure.dpi'] = 800\n",
    "rcParams['axes.titlesize'] = 12\n",
    "rcParams['xtick.labelsize'] = 10\n",
    "rcParams['ytick.labelsize'] = 14\n",
    "rcParams['font.size'] = 7.5\n",
    "\n",
    "sns.set_theme(style='whitegrid')\n",
    "\n",
    "ax = sns.countplot(data=df[df['Category'].isin(class_order_2)], y='Category', hue='Set', order=class_order_2, orient='v')\n",
    "ax.set_yticklabels(name_2lines(class_order_2, 30), ha='right', va='center', rotation=0, fontsize=14)\n",
    "ax.set_ylabel(None)\n",
    "ax.set_xlabel(None)\n",
    "\n",
    "before=0\n",
    "for y, size in super_size_2.iteritems():\n",
    "    plt.text(-25, before-0.5 + size/2, name_2lines(y), ha='right', va='center', fontsize=14, weight='bold')\n",
    "    before += size\n",
    "plt.xlim(0, 60)\n",
    "plt.legend(fontsize='x-large', loc='lower right')\n",
    "plt.savefig(f'Plots/fing_validation_2.png', bbox_inches='tight', transparent=True, dpi=800)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2d3f06b",
   "metadata": {},
   "outputs": [],
   "source": [
    "gs = pickle.load(open(f'Models/Subclass_flat_RF.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21da1a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('f1_macro_test ->', gs.cv_results_['mean_test_f1_macro'][gs.best_index_])\n",
    "print('f1_macro_train ->', gs.cv_results_['mean_train_f1_macro'][gs.best_index_])\n",
    "print('f1_micro_test ->', gs.cv_results_['mean_test_f1_micro'][gs.best_index_])\n",
    "print('f1_micro_train ->', gs.cv_results_['mean_train_f1_micro'][gs.best_index_])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
